---
title: "models"
output: html_document
---

```{r, warning = FALSE}
library(readr)
library(ggplot2)
library(tidyr)
library(MASS)
library(leaps)
library(glmnet)
library(caret)
library(rpart)
library(rpart.plot)
library(randomForest)
library(gbm)
library(pls)
library(ivreg)
library(AER)
```

### load datasets
```{r}
train_data <- read_csv("train_data.csv")
test_data <- read_csv("test_data.csv")
data <- read_csv("data.csv")

train_control <- trainControl(method = "cv", number = 10)
```

```{r, warning = FALSE}
#Training Models
set.seed(4305)
#1. Baseline Model: OLS
model_ols <- train(medv ~ ., data = data, 
                   method = "lm", 
                   trControl = train_control)

#2. Ridge Regression
model_ridge <- train(medv ~ ., data = data, 
                     method = "glmnet", 
                     trControl = train_control,
                     preProcess = c("center", "scale"),
                     tuneGrid = expand.grid(alpha = 0, lambda = seq(0.01, 1, by = 0.05)))

#3. Lasso Regression
model_lasso <- train(medv ~ ., data = data, 
                     method = "glmnet", 
                     trControl = train_control,
                     preProcess = c("center", "scale"),
                     tuneGrid = expand.grid(alpha = 1, lambda = seq(0.01, 1, by = 0.05)))

#4. Elastic Net Regression
model_elastic <- train(medv ~ ., data = data, 
                     method = "glmnet", 
                     trControl = train_control,
                     preProcess = c("center", "scale"),
                     tuneGrid = expand.grid(alpha = 0.5, lambda = seq(0.01, 1, by = 0.05)))

#5. Stepwise (Backwards)
model_bwd <- train(medv ~ ., data = data, 
                   method = "leapBackward", 
                   trControl = train_control,
                   tuneGrid = data.frame(nvmax = 1:13))

#6. Stepwise (Forward)
model_fwd <- train(medv ~ ., data = data, 
                   method = "leapForward", 
                   trControl = train_control,
                   tuneGrid = data.frame(nvmax = 1:13))

#7. Polynomial Regression 
# For non-linear variables identified in EDA (lstat and rm)
model_poly <- train(medv ~ poly(lstat, 2) + poly(rm, 2) + crim + zn + 
                      indus + chas + nox + age + dis + rad + tax + ptratio + black, 
                    data = data, 
                    method = "lm", 
                    trControl = train_control)

#8. K-Nearest neighbours
model_knn <- train(medv ~ ., data = data, 
                   method = "knn", 
                   preProcess = c("center", "scale"),
                   trControl = train_control,
                   tuneLength = 10)

#9. Decision Tree
model_tree <- train(medv ~ ., data = data,
                    method = "rpart",
                    trControl = train_control,
                    tuneLength = 10)

#10. Random Forest
model_rf <- train(medv ~ ., data = data,
                  method = "rf",
                  trControl = train_control,
                  tuneLength = 5,
                  importance = TRUE)

#11. GBM
model_gbm <- train(medv ~ ., data = data,
                   method = "gbm",
                   trControl = train_control,
                   verbose = FALSE,
                   tuneLength = 5)

#12. PCR
model_pcr <- train(medv ~ ., data = data,
                   method = "pcr",
                   trControl = train_control,
                   preProcess = c("center", "scale"),
                   verbose = FALSE,
                   tuneLength = 13)

#13. PLS
model_pls <- train(medv ~ ., data = data,
                   method = "pls",
                   trControl = train_control,
                   preProcess = c("center", "scale"),
                   verbose = FALSE,
                   tuneLength = 13)

#14. IV
ols <- lm( medv ~ lstat + dis + rm + rad + ptratio + nox + tax, data)

fsls <- lm(lstat ~ dis + rm + rad + ptratio + nox + tax + age + crim + zn + indus + black, data)

summary(fsls)

#age as instrument for lstat
iv1 <- ivreg(
  medv ~ lstat + dis + rm + rad + ptratio + nox + tax  
  |age + dis + rm + rad + ptratio + nox + tax, data=data)

summary(iv1, diagnostics = TRUE)
m_list1 <- list(ols, iv1)
msummary(m_list1)

#crim as instrument for lstat
iv2 <- ivreg(
  medv ~ lstat + dis + rm + rad + ptratio + nox + tax  
  |crim + dis + rm + rad + ptratio + nox + tax, data=data)

summary(iv2, diagnostics = TRUE)
m_list1 <- list(ols, iv2)
msummary(m_list2)

```

### Testing Models
```{r}
#consolidating CV for each model
results <- resamples(list(
  OLS = model_ols,
  Ridge = model_ridge,
  Lasso = model_lasso,
  ElasticNet = model_elastic,
  ForwardStep = model_fwd,
  BackwardStep = model_bwd,
  Polynomial = model_poly,
  KNN = model_knn,
  Tree = model_tree,
  RandomForest = model_rf,
  GBM = model_gbm,
  PCR = model_pcr,
  PLS = model_pls
))

summary(results)

#RMSE
model_rmse_mean <- summary(results)$statistics$RMSE[,"Mean"]
rmse_summary <- data.frame(Model = names(model_rmse_mean),
                           Mean_RMSE = model_rmse_mean)
rmse_summary[order(rmse_summary$Mean_RMSE), ]


#R-squared
model_r2_mean <- summary(results)$statistics$Rsquared[,"Mean"]
r2_summary <- data.frame(Model = names(model_r2_mean),
                         Mean_Rsquared = model_r2_mean)
r2_summary[order(r2_summary$Mean_Rsquared, decreasing = TRUE), ]


#MAE
model_mae_mean <- summary(results)$statistics$MAE[,"Mean"]
mae_summary <- data.frame(Model = names(model_mae_mean),
                          Mean_MAE = model_mae_mean)
mae_summary[order(mae_summary$Mean_MAE), ]

#Manual CV for IV Regression
K <- 10
folds <- sample(rep(1:K, length.out = nrow(data)))

rmse_iv <- numeric(K)

for (k in 1:K) {

  # Split data
  train_idx <- which(folds != k)
  test_idx  <- which(folds == k)

  train <- data[train_idx, ]
  test  <- data[test_idx, ]

  # Fit IV model on training fold
  iv_k <- iv

  # Predict on test fold
  preds <- predict(iv_k, newdata = test)

  # Compute RMSE for this fold
  rmse_iv[k] <- sqrt(mean((test$medv - preds)^2))
}

# RMSE for each fold
rmse_iv

# Mean RMSE across folds
mean(rmse_iv)

```

```{r}
#interpretation of coefficients
summary(model_ols$finalModel)
summary(model_poly$finalModel)
summary(model_fwd$bestTune)
summary(model_bwd$finalModel)

#Visualize the Tree
rpart.plot(model_tree$finalModel)

#PCR Analysis
final_pcr <- model_pcr$finalModel
validationplot(final_pcr, val.type="RMSEP", main="10-fold CV")
summary(final_pcr)
round(coef(final_pcr, ncomp = 5), 3)
final_pcr$method

#PLS Analysis
final_pls <- model_pls$finalModel
#Plot 10-fold CV MSE's:
validationplot(final_pls, val.type="RMSEP", main="PLS 10-fold CV")
summary(final_pls)
round(coef(final_pls, ncomp = 2), 3)
final_pls$method

# Variable Importance 
plot(varImp(model_ols), main = "OLS Variable Importance")
plot(varImp(model_lasso), main = "Lasso Feature Selection")
plot(varImp(model_ridge), main = "Ridge Shrinkage")
plot(varImp(model_poly), main = "Polynomial Model Importance")
plot(varImp(model_tree), main = "Decision Tree Variable Importance")
plot(varImp(model_rf), main = "Random Forest Variable Importance")
plot(varImp(model_gbm), main = "GBM Variable Importance")
```
